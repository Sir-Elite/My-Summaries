{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is about learning the machine how to predict or evaluate the outcome of a given input. It involves algorithms and techniques of its own to train and test the results on different types of data. It works with voices and speech recognition, text and languages, images and videos, tabular data and numbers, electrical signals, ... etc.\n",
    "\n",
    "In other words by Arthur Samuel, \"Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.\"\n",
    "\n",
    "The main two machine learning algorithms are:\n",
    "* Supervised Machine Learning which is the type used most in real-world applications and has seen the most rapid advancements and innovation.\n",
    "* Unsupervised Machine Learning which is the second most used type of machine learning.\n",
    "\n",
    "There are many other machine learning algorithms like recommender systems and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of some of the notation you will encounter.  \n",
    "\n",
    "|General <br /> Notation| Description | Python (if applicable) |\n",
    "|-------------------|----------------------------------------------------------------------------------------------------|-------------------------|\n",
    "| $a$  | scalar, non bold | |\n",
    "| $\\mathbf{a}$  | vector, bold  | |\n",
    "| **Regression** |  |  |\n",
    "|  $\\mathbf{x}$  | Training Example feature values <img width=429/> | `x_train` | \n",
    "|  $\\mathbf{y}$  | Training Example  targets  | `y_train` | \n",
    "|  $x^{(i)}$, $y^{(i)}$  | ith Training Example  | `x[i]`, `y[i]` |\n",
    "|  $x_j$  | The value of jth Training feature  | `x[:,j]` |\n",
    "|  ${x_j}^{(i)}$  | The value of the jth Training feature in the ith training example  | `x[i,j]` |\n",
    "| m  | Number of training examples | `m` |\n",
    "| n | Total number of features | `n`|\n",
    "|  $w$ |  parameter: weight | `w` |\n",
    "| $w_{(i)}$ | The ith weight of a multiple regression equation | `w_i`|\n",
    "|  $b$ |  parameter: bias | `b` |\n",
    "| $\\alpha$ | The learning rate; which is usually a small number ($0 < \\alpha < 1$) | `alpha` | \n",
    "| $\\hat{y}^{(i)}$ | predicted target value | `y_pred` |   \n",
    "| $f_{w,b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$  | `f_wb`|\n",
    "\n",
    "; where $f_{w,b}(x^{(i)}) = \\hat{y}^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised machine learning algorithms, the learning algorithm deals with unlabeled X (input feature(s)). The algorthim has to find some structure or some pattern or something interesting in the data. Major types of supervised machine learning algorithms are:\n",
    "1. Clustering: Grouping similar input data points together within unlabeled categories or classes called clusters.\n",
    "2. Anomaly Detecting: Finding unusual data points. It is really important for fraud detection in the financial system, where unusual events, unusual transactions could be signs of fraud and for many other applications.\n",
    "3. Dimensionality Reduction: Compressing data using fewer numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of machine learning algorithms, you give the learning algorithm examples to learn from. These examples are presented as X (input, which can contain more than  one feature) and Y (output which is always one label). It learns from being given the \"right answers\" (correct Y labels). Major types of supervised machine learning algorithms are:\n",
    "1. Regression: predicting a number from an infinitely many possible outputs based on some given data.\n",
    "2. Classification: predicting a category or a class of an input, all of a small set of possible outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Let's use the same two data points as before - a house with 1000 square feet sold for \\\\$300,000 and a house with 2000 square feet sold for \\\\$500,000.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| ----------------| ------------------------ |\n",
    "| 1               | 300                      |\n",
    "| 2               | 500                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Univariant Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center> <img src=\"./images/C1_W1_L3_S1_Lecture_b.png\"  style=\"width:900px;height:300px;\" ></center>\n",
    "</figure>\n",
    "\n",
    "In the linear regression, the prediction output is a line of equation $f_{w,b}(x^{(i)}) = wx^{(i)}+b$; where w and b are the parameters to be optimized, they are also called weight and bias. Sometimes, they are both called weights or coefficients of the linear model function. These weights affect the position and the slope of the line produced.\n",
    "\n",
    "The Goal from regression models is to get the best fitting line with the least amount of errors between the predicted data ($\\hat{y}^{(i)}$) to the actual data ($y^{(i)}$), or minimizing the cost function ($J(w,b)$). To Acheive such goal we can use \"Gradient Descent\" algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_output(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the prediction of a linear model\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      f_wb (ndarray (m,)): model prediction\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    f_wb = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        f_wb[i] = w * x[i] + b\n",
    "        \n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function measures the difference between the model's predictions ($\\hat{y}^{(i)}$) and the actual measures ($y^{(i)}$). In machine learning different people will use different cost functions for different applications, but the squared error cost function is by far the most commonly used one for linear regression and for that matter, for all regression problems where it seems to give good results for many applications.\n",
    "\n",
    "The square error cost function:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)})^2 \\tag{1}$$ \n",
    "\n",
    "Kowing that the error is:\n",
    "\n",
    "$$error = \\hat{y}^{(i)} - y^{(i)}$$\n",
    "\n",
    "Example on square error cost function which is a convex function (looks like a super bowl, because it has only one minimum):\n",
    "\n",
    "<figure>\n",
    "    <center> <img src=\"./images/Cost Function_Linear Regression.png\"  style=\"width:650px;height:400px;\" ></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, gradient descent applies to many general functions, including other cost functions (other than square error cost function) that work with models that have more than two parameters. It turns out that gradient descent is an algorithm that you can apply to try to minimize this cost function j. It works with linear regression by:\n",
    "1.  Setting an initial guess for `w` and `b`.   $\\newline$  In linear regression, it won't matter too much what the initial value are, so a common choice is to set them both to 0.\n",
    "2.  Choosing a proper learning rate ($\\alpha$)\n",
    "3.  Calculating the gradients\n",
    "4.  Updating the values of `b` and `w` simultaneously\n",
    "5.  Repeat the steps 3 and 4 until convergence (gradient descent's algorithm). In other words, keep on changing the parameters `w` and `b` a bit every time to try to reduce the cost ( $J(w,b)$ ) until hopefully j settles at or near a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Algorithm Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the values of $w$ and $b$ for many iteration, repeat the following steps of upadating until convergence, which is reaching the point at a local minimum where the parameters w and b no longer change much with each additional step taken:\n",
    "\n",
    "$$\\begin{align*} \\;\n",
    "\\;  w &= w -  \\alpha  \\frac{\\partial J(w,b)}{\\partial w} \\tag{2}  \\; \\newline \n",
    " b &= b -  \\alpha  \\frac{\\partial J(w,b)}{\\partial b} \\tag{3}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where, parameters $w$, $b$ are updated simultaneously using the gradient defined using the J's derivative defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate ($\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate ($\\alpha$) controls how big is a step the algorithm will take towards the minimum, and the derivative estimates in which direction the steps will be taken. There are 3 senarios in choosing the $\\alpha$:\n",
    "1.  $\\alpha$ is too small, so gradient descent may be slower.\n",
    "2.  $\\alpha$ is too big, so gradient descent may diverge. In other words, it may overshoot and never hit the minimum.\n",
    "3.  $\\alpha$ is in optimal range.\n",
    "\n",
    "Increasing the learning rate may result in bigger steps and reaching faster, but it can result also in a learning rate that is too large, and cause gradient descent to fail to find the optimal values for the parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various types of gradient descent like:\n",
    "1.  Batch Gradient Descent (the one working on the whole training set like the one we are using now; i= 0 : m-1 .)\n",
    "2.  Mini-Batch Gradient Descent (works with subset of the trainig set)\n",
    "3.  Stochastic Gradient Descent (randomly selects a training example, computes the gradient of the cost function for that example, and updates the parameters in the opposite direction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):                      # Iterate over all training samples\n",
    "        f_wb = w * x[i] + b                 # Calculate the predicted value\n",
    "        # Calculate the partial derivatives (gradients) of the current point\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = f_wb - y[i]\n",
    "        # Add them all together\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    # Get the average partial derivatives (gradients) to be used in value updating\n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "    \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later (extra part just to inspect values and steps)\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "############################################################################################## impelementation code body start + the loop itself\n",
    "        # Update Parameters using equations (2) & (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "############################################################################################## impelementation code body end + the loop itself\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:                               # prevent resource exhaustion and appending (saving) no more data into the lists to inspect\n",
    "            J_history.append(cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10 to inspect too.\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 7.93e+04  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  6.500e+00, b: 4.00000e+00\n",
      "Iteration 1000: Cost 3.41e+00  dj_dw: -3.712e-01, dj_db:  6.007e-01   w:  1.949e+02, b: 1.08228e+02\n",
      "Iteration 2000: Cost 7.93e-01  dj_dw: -1.789e-01, dj_db:  2.895e-01   w:  1.975e+02, b: 1.03966e+02\n",
      "Iteration 3000: Cost 1.84e-01  dj_dw: -8.625e-02, dj_db:  1.396e-01   w:  1.988e+02, b: 1.01912e+02\n",
      "Iteration 4000: Cost 4.28e-02  dj_dw: -4.158e-02, dj_db:  6.727e-02   w:  1.994e+02, b: 1.00922e+02\n",
      "Iteration 5000: Cost 9.95e-03  dj_dw: -2.004e-02, dj_db:  3.243e-02   w:  1.997e+02, b: 1.00444e+02\n",
      "Iteration 6000: Cost 2.31e-03  dj_dw: -9.660e-03, dj_db:  1.563e-02   w:  1.999e+02, b: 1.00214e+02\n",
      "Iteration 7000: Cost 5.37e-04  dj_dw: -4.657e-03, dj_db:  7.535e-03   w:  1.999e+02, b: 1.00103e+02\n",
      "Iteration 8000: Cost 1.25e-04  dj_dw: -2.245e-03, dj_db:  3.632e-03   w:  2.000e+02, b: 1.00050e+02\n",
      "Iteration 9000: Cost 2.90e-05  dj_dw: -1.082e-03, dj_db:  1.751e-03   w:  2.000e+02, b: 1.00024e+02\n",
      "(w,b) found by gradient descent: (199.9929,100.0116)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For some functions ( $J(w_{(1)},w_{(2)},...,w_{(n)},b)$ ) that may not be a bow shape or a hammock shape, it is possible for there to be more than one possible minimum, like those for a neural network. In such type of functions, different initial positions result in different weights. Example:\n",
    "\n",
    "<figure>\n",
    "    <left> <img src=\"./images/Cost Function_Neural Network.png\"  style=\"width:560px;height:350px;\" ></left>\n",
    "    <right> <img src=\"./images/Cost Function_Neural Network + Gradient Descent Steps.png\"  style=\"width:540px;height:400px;\" ></right>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If we pick initial values for the weights to be at a local minimum, the gradients will be zeros. Therefore, going back to equations (2) and (3), the values will not be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The cost decreases as you continue training more iterations. In other words, the cost gets closer to the global minimum as gradient descent finds better and better values for the parameters `w` and `b`. At the same time, gradient descent finds better values for the parameters `w` and `b`, as the cost gets closer to the global minimum. And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.   As we approach the local minimum, the steps get smaller because the gradients too get smaller."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
